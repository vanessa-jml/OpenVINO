## Overview of Intel® Distribution of OpenVINO™ Toolkit

AI inference applies capabilities learned after training a neural network to yield results. The [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/content/www/us/en/develop/tools/openvino-toolkit.html) enables you to optimize, tune, and run comprehensive AI inference using the included model optimizer and runtime and development tools.

![Image of OpenVino](https://software.intel.com/content/dam/develop/public/us/en/images/diagrams-infographics/diagram-v1openvino-homepage-16x9.png)

## Discover the Capabilities

### High Performance, Deep Learning

Convert and optimize models to achieve high performance for deep-learning inference applications.

### Streamlined Development

Facilitate a smoother development process using the included inference tools for low-precision optimization and media processing, computer vision libraries, and preoptimized kernels.

### Write Once, Deploy Anywhere

Deploy your same application across combinations of host processors, accelerators, and environments, including CPUs, GPUs, VPUs, FPGAs, on-premise and on-device, and in the browser or in the cloud.
